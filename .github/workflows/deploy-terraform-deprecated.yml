name: "[DEPRECATED] Deploy to AWS S3 with Terraform"
# ‚ö†Ô∏è  DEPRECATED: This workflow is deprecated in favor of deploy-pulumi.yml
# This file is kept for reference but should not be used for new deployments
permissions:
  id-token: write
  contents: read

on:
  # Disabled - use deploy-pulumi.yml instead
  workflow_dispatch: # Manual trigger only

jobs:
  build-and-test:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4.2.2

    - name: Setup Node.js
      uses: actions/setup-node@v4.4.0
      with:
        node-version: '24'
        cache: 'npm'

    - name: Install playwright && machine dependencies
      run: npx playwright install --with-deps


    - name: Install dependencies
      run: npm install

    - name: Run tests (CI optimized)
      run: npm run test:ci

    - name: Build application in a static setup
      run: npm run build:static

    - name: Upload production build
      uses: actions/upload-artifact@v4.6.2
      with:
        name: production-build
        path: build/

  deploy:
    needs: build-and-test
    runs-on: ubuntu-latest
    environment: production
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4.2.2
        
      - name: Download production build
        uses: actions/download-artifact@v4.1.7
        with:
          name: production-build
          path: build/

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ~1.7

      - name: Terraform Init
        working-directory: ./infra/terraform
        run: terraform init

      - name: Apply Basic Infrastructure with Import Handling
        working-directory: ./infra/terraform
        run: |
          set +e  # Don't exit on error, we want to handle it
          echo "üéØ Applying basic infrastructure first..."
          
          # Try targeted apply first - this will fail if bucket exists but isn't imported
          terraform apply -target=aws_s3_bucket.website -target=aws_s3_bucket_website_configuration.website -target=aws_s3_bucket_public_access_block.website -target=aws_s3_bucket_policy.website -target=aws_acm_certificate.website -auto-approve
          APPLY_RESULT=$?
          
          if [ $APPLY_RESULT -ne 0 ]; then
            echo "‚ö†Ô∏è  Apply failed - likely due to existing S3 bucket. Attempting import..."
            
            # Check if S3 bucket exists and import it
            if aws s3api head-bucket --bucket ${{ secrets.DOMAIN_NAME }} 2>/dev/null; then
              echo "‚úÖ S3 bucket exists, importing..."
              terraform import aws_s3_bucket.website ${{ secrets.DOMAIN_NAME }}
              
              # Now retry the targeted apply
              echo "üîÑ Retrying basic infrastructure apply..."
              set -e  # Re-enable exit on error for the retry
              terraform apply -target=aws_s3_bucket.website -target=aws_s3_bucket_website_configuration.website -target=aws_s3_bucket_public_access_block.website -target=aws_s3_bucket_policy.website -target=aws_acm_certificate.website -auto-approve
            else
              echo "‚ùå Apply failed and bucket doesn't exist - this is unexpected"
              set -e
              exit 1
            fi
          else
            echo "‚úÖ Basic infrastructure applied successfully"
          fi
        env:
          TF_VAR_aws_region: ${{ secrets.AWS_REGION }}
          TF_VAR_domain_name: ${{ secrets.DOMAIN_NAME }}

      - name: Terraform Plan (Full)
        working-directory: ./infra/terraform
        run: terraform plan
        env:
          TF_VAR_aws_region: ${{ secrets.AWS_REGION }}
          TF_VAR_domain_name: ${{ secrets.DOMAIN_NAME }}

      - name: Terraform Apply (Full) with CloudFront Import Handling
        working-directory: ./infra/terraform
        run: |
          set +e  # Don't exit on error, we want to handle it
          echo "üöÄ Applying full infrastructure..."
          
          # Try full apply - this may fail if CloudFront distribution exists
          terraform apply -auto-approve
          APPLY_RESULT=$?
          
          if [ $APPLY_RESULT -ne 0 ]; then
            echo "‚ö†Ô∏è  Full apply failed - checking for existing CloudFront distribution..."
            
            # Find existing CloudFront distribution for this domain
            DISTRIBUTION_ID=$(aws cloudfront list-distributions --query "DistributionList.Items[?contains(Aliases.Items, '${{ secrets.DOMAIN_NAME }}')].Id | [0]" --output text 2>/dev/null || echo "None")
            
            if [[ "$DISTRIBUTION_ID" != "None" && "$DISTRIBUTION_ID" != "null" && "$DISTRIBUTION_ID" != "" ]]; then
              echo "‚úÖ CloudFront distribution exists: $DISTRIBUTION_ID, importing..."
              terraform import aws_cloudfront_distribution.website "$DISTRIBUTION_ID"
            fi
            
            # Check for existing Route53 records and import them
            echo "üîç Checking for existing Route53 records..."
            HOSTED_ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name "${{ secrets.DOMAIN_NAME }}" --query "HostedZones[?Name=='${{ secrets.DOMAIN_NAME }}.'].Id | [0]" --output text 2>/dev/null | sed 's|/hostedzone/||')
            
            if [[ "$HOSTED_ZONE_ID" != "None" && "$HOSTED_ZONE_ID" != "null" && "$HOSTED_ZONE_ID" != "" ]]; then
              echo "‚úÖ Found hosted zone: $HOSTED_ZONE_ID"
              
              # List existing records for debugging
              echo "üìù Existing A records in zone:"
              aws route53 list-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" --query "ResourceRecordSets[?Type=='A'].{Name:Name,Type:Type}" --output table || true
              
              # Import main domain A record if it exists
              if aws route53 list-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" --query "ResourceRecordSets[?Name=='${{ secrets.DOMAIN_NAME }}.' && Type=='A']" --output text | grep -q .; then
                echo "‚úÖ Main domain A record exists, importing..."
                terraform import aws_route53_record.website "${HOSTED_ZONE_ID}_${{ secrets.DOMAIN_NAME }}_A" || echo "‚ö†Ô∏è Main domain record import failed (continuing)"
              fi
              
              # Import www subdomain A record if it exists  
              if aws route53 list-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" --query "ResourceRecordSets[?Name=='www.${{ secrets.DOMAIN_NAME }}.' && Type=='A']" --output text | grep -q .; then
                echo "‚úÖ WWW subdomain A record exists, importing..."
                terraform import aws_route53_record.website_www "${HOSTED_ZONE_ID}_www.${{ secrets.DOMAIN_NAME }}_A" || echo "‚ö†Ô∏è WWW record import failed (continuing)"
              fi
            else
              echo "‚ö†Ô∏è No hosted zone found for ${{ secrets.DOMAIN_NAME }}"
            fi
            
            # Now retry the full apply
            echo "üîÑ Retrying full infrastructure apply..."
            set -e  # Re-enable exit on error for the retry
            terraform apply -auto-approve
          else
            echo "‚úÖ Full infrastructure applied successfully"
          fi
        env:
          TF_VAR_aws_region: ${{ secrets.AWS_REGION }}
          TF_VAR_domain_name: ${{ secrets.DOMAIN_NAME }}

      - name: Sync files to S3
        run: |
          aws s3 sync build/ s3://${{ secrets.DOMAIN_NAME }}/ --delete --exact-timestamps
          
      - name: Invalidate CloudFront
        run: |
          DISTRIBUTION_ID=$(aws cloudfront list-distributions --query "DistributionList.Items[?Aliases.Items[0]=='${{ secrets.DOMAIN_NAME }}'].Id" --output text)
          aws cloudfront create-invalidation --distribution-id $DISTRIBUTION_ID --paths "/*" 
